

### Classification

estimate a discrete variable for each input

### Regression

compute a value for a variable   

 ---

*See slides to see difference between last activation and loss function*
### maximum  likelyhood estimation

Maximum Likelihood Estimation (MLE) is a fundamental method used in deep learning and statistics to estimate the parameters of a statistical model. The basic idea behind MLE is to find the set of parameters that maximizes the likelihood function, which measures how likely it is to observe the given data under different parameter values. 

loss functions can be placed into to be probabilistic framework. set of observations with associated labels x and y

require a conditional probability density function to define p(y/x) relation

$$

\prod_{m=1}^M p(y_m|x_m)

$$
Independent and Identically Distributed

High and low values can cancel out Quiickly

 we can look at *Negative LIkelihood* : Then our large product is a sum of all conditions with negative logarhtim of the condtional probablities

*See slide 5 for equation for negative loss likelihood*

**Computing log likelyhood for Univerariate Gaussian Model**
A univariate Gaussian model, also known as a univariate normal distribution, is characterized by two parameters: the mean (μ) and the variance (σ²). The probability density function (pdf) of a univariate Gaussian distribution is given by:

$$f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right) $$


 $$
 L(w, \beta) = \prod_{i=1}^N p(y_i \mid x_i, w, \beta) 
$$
Substituting the Gaussian into the likelihood function:
$$

 L(w, \beta) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi \frac{1}{\beta}}} \exp\left( -\frac{\beta (y_i - x_i^T w)^2}{2} \right) 
$$

$$
log L(w, \beta) = \sum_{i=1}^N \left[ -\frac{1}{2} \log (2\pi) + \frac{1}{2} \log (\beta) - \frac{\beta (y_i - x_i^T w)^2}{2} \right] 
$$


$$\log L(w, \beta) = -\frac{N}{2} \log (2\pi) + \frac{N}{2} \log (\beta) - \frac{\beta}{2} \sum_{i=1}^N (y_i - x_i^T w)^2 $$

L2 loss and L1 loss can be used for classification: corresponds to minimizing misclassification probs. can cause slow convergance because dont penalize heavily misclassified preds. adv in label noise

The L2 loss arises naturally when assuming Gaussian-distributed errors in the data. Minimizing the L2 loss is equivalent to performing maximum likelihood estimation under this assumption.

---
To put it simply, the log-likelihood estimator is used to compute the loss function in many machine learning and deep learning models, particularly those involving probabilistic frameworks.

### Breakdown of the Concept

1. **Likelihood Function**: The likelihood function represents the probability of observing the given data under specific model parameters. In other words, it measures how well the model explains the observed data.

2. **Log-Likelihood**: The log-likelihood is simply the natural logarithm of the likelihood function. It is often used because it simplifies the mathematical computations involved, especially when dealing with products of probabilities (which become sums of log-probabilities).

3. **Loss Function**: The loss function is what we aim to minimize during the training process. In many models, the loss function is derived from the negative log-likelihood. Minimizing the negative log-likelihood is equivalent to maximizing the likelihood, leading to parameter estimates that best explain the observed data.

### How It Works in Practice

- **Classification**: For example, in logistic regression or neural networks for classification, the cross-entropy loss function used is derived from the negative log-likelihood of the predicted probabilities.

- **Regression**: In regression tasks assuming Gaussian error distribution, the mean squared error (MSE) can be viewed as being derived from the negative log-likelihood of the Gaussian distribution.

- **Generative Models**: Models like Variational Autoencoders (VAEs) involve maximizing a bound on the log-likelihood to learn the distribution of the data.

### Example in Classification

For a binary classification problem, the model predicts the probability \( P(y=1 | X, \theta) \). The log-likelihood for the observed labels given the inputs is:
$$ \log L(\theta; X, y) = \sum_{i} \left[ y_i \log P(y_i=1 | X_i, \theta) + (1 - y_i) \log (1 - P(y_i=1 | X_i, \theta)) \right] $$

The corresponding loss function, which is the negative log-likelihood, is:
\[ \text{Loss} = -\log L(\theta; X, y) \]

### Summary

- **Log-Likelihood Estimator**: Used to measure how well the model parameters explain the observed data.
- **Log-Likelihood in Loss Function**: The loss function is often derived from the negative log-likelihood, and minimizing this loss function is a key part of training the model.
- **Backpropagation and Optimization**: Backpropagation and gradient descent are used to iteratively adjust the model parameters to minimize this loss function.

In essence, the log-likelihood estimator provides a foundation for defining the loss function, guiding the learning process to find the best model parameters.

---

### L2 loss

L2 loss, also known as the Mean Squared Error (MSE), is a common loss function used in regression tasks. It measures the average of the squared differences between predicted and actual values. The L2 loss function penalizes larger errors more than smaller ones, which makes it useful for many regression problems. Here’s why MSE is used:

1. **Error Calculation**: It measures the difference between the predicted value and the actual value.
2. **Squaring Errors**: Squaring the errors ensures that both positive and negative differences contribute to the loss, and larger errors have a bigger impact.
3. **Average of Errors**: Taking the average helps to generalize the error across all predictions.

### Why Use MSE?
Using MSE makes sense in many regression problems because it assumes that the errors (differences between predicted and actual values) are normally distributed (bell-shaped curve).

### Link to Log-Likelihood
The MSE loss can be seen as related to the negative log-likelihood of the Gaussian (normal) distribution. Here's a simplified explanation:

1. **Gaussian Distribution**: If we assume the errors follow a normal distribution, the likelihood of the observed data given the model’s predictions is higher when the errors are smaller.
2. **Log-Likelihood**: The log-likelihood of the data under this assumption involves a term that looks very similar to the MSE.

### Using MSE in Loss Function
To train a regression model, we aim to minimize the MSE loss. Here’s how it works:

1. **Predicted Values**: The model outputs a predicted value for each input.
2. **Calculate Errors**: For each prediction, compute the error by subtracting the predicted value from the actual value.
3. **Square Errors**: Square these errors to ensure all differences are positive and to emphasize larger errors.
4. **Average Errors**: Compute the average of these squared errors across all predictions.

---
### L1 loss

L1 loss, also known as the Mean Absolute Error (MAE), is another common loss function used in regression tasks. Unlike the Mean Squared Error (MSE), which penalizes larger errors more heavily due to squaring, L1 loss penalizes errors linearly. Let's break down how L1 loss is derived, its formulation, and its usage:

---
In deep learning for classification tasks, the Bernoulli distribution is commonly used to model binary outcomes, where the target variable \( y \) takes on one of two possible values, typically encoded as 0 and 1. Let's delve into how the Bernoulli distribution is applied in this context:

### Bernoulli Distribution Basics

The Bernoulli distribution represents a single trial with two possible outcomes: success (usually denoted as 1) with probability \( p \), and failure (denoted as 0) with probability \( 1 - p \). The probability mass function (PMF) of the Bernoulli distribution is:

$$
P(Y=y) = \begin{cases}
p & \text{if } y = 1 \\
1-p & \text{if } y = 0
\end{cases}
$$

Where:
- \( Y \) is the random variable representing the outcome (0 or 1).
- \( p \) is the probability of success (the probability that \( Y = 1 \)).

### Application in Deep Learning for Classification

In deep learning for binary classification tasks, the Bernoulli distribution is often used to model the probability of a certain class label (e.g., class 1) given input features \( X \) and model parameters \( \theta \). The goal is to learn the parameters \( \theta \) that best describe the data.

### Logistic Regression

One common method that utilizes the Bernoulli distribution is logistic regression. In logistic regression, we model the probability of the positive class (class 1) as a function of the input features:

$$
P(Y=1 | X, \theta) = \sigma(\theta^T X)
$$

Where:
- \( \sigma \) is the sigmoid function, which maps real-valued outputs to the range [0, 1].
- \( X \) is the input features.
- \( \theta \) is the parameter vector (weights) to be learned.

The probability of the negative class (class 0) is then \( P(Y=0 | X, \theta) = 1 - P(Y=1 | X, \theta) \).

### Training

During training, the parameters \( \theta \) are learned by maximizing the likelihood of the observed data. For a dataset with \( N \) examples, the likelihood function can be written as:

$$
L(\theta; X, y) = \prod_{i=1}^{N} P(Y=y_i | X_i, \theta)
$$

Taking the logarithm of the likelihood function (to convert the product into a sum) gives the log-likelihood:

$$
\log L(\theta; X, y) = \sum_{i=1}^{N} \left[ y_i \log P(Y=1 | X_i, \theta) + (1 - y_i) \log P(Y=0 | X_i, \theta) \right]
$$

The negative log-likelihood (which needs to be minimized) is the loss function used in training logistic regression models:

$$
\text{Loss} = -\log L(\theta; X, y)
$$

Gradient descent or other optimization algorithms are then employed to minimize this loss function, adjusting the parameters \( \theta \) to improve the model's predictions.

---

Sure, here is the content with the equations in block format using markdown:

---

Cross-entropy loss and Kullback-Leibler (KL) divergence are both fundamental concepts in information theory and machine learning, often used in the context of classification problems. To understand their relationship, we need to delve into their definitions and derivations.

### Bernoulli Distribution and Cross-Entropy Loss

In binary classification, we can model the problem using a Bernoulli distribution. Let \( y \in \{0, 1\} \) be the true label and \( \hat{y} \in [0, 1\} \) be the predicted probability of \( y = 1 \).

The probability mass function of the Bernoulli distribution for the true label \( y \) is given by:
$$
P(y) = p^y (1 - p)^{1-y}
$$

When \( y \) is 1, \( P(y) = p \). When \( y \) is 0, \( P(y) = 1 - p \).

The predicted probability \( \hat{y} \) represents our model's estimate of \( p \). The cross-entropy loss for a single data point is defined as the negative log-likelihood of the true label given the predicted probability:
$$
\text{Cross-Entropy Loss} = -[y \log(\hat{y}) + (1-y) \log(1 - \hat{y})]
$$

### Kullback-Leibler Divergence

KL divergence measures the difference between two probability distributions \( P \) and \( Q \). For discrete distributions, it is defined as:
$$
D_{KL}(P \parallel Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}
$$

In the context of a binary classification problem, we are interested in the divergence between the true distribution \( P \) (represented by the true label \( y \)) and the predicted distribution \( Q \) (represented by the predicted probability \( \hat{y} \)).

### Relationship Between Cross-Entropy Loss and KL Divergence

To understand the relationship between cross-entropy loss and KL divergence, consider the expected value of the cross-entropy loss over the true distribution \( P \):
$$
H(P, Q) = -\mathbb{E}_{P}[\log Q(x)]
$$

For binary classification, this expectation can be expressed as:
$$
H(P, Q) = -[y \log(\hat{y}) + (1-y) \log(1 - \hat{y})]
$$

The entropy of the true distribution \( P \), denoted as \( H(P) \), is the expected value of the negative log-likelihood of the true distribution itself:
$$
H(P) = -\mathbb{E}_{P}[\log P(x)]
$$

For a Bernoulli distribution with true label \( y \), this is:
$$
H(P) = -[y \log(y) + (1-y) \log(1 - y)]
$$

However, since \( y \) is either 0 or 1, this simplifies to 0, as \( \log(1) = 0 \) and \( y \log(y) \) or \( (1-y) \log(1-y) \) will always be 0.

The KL divergence between the true distribution \( P \) and the predicted distribution \( Q \) can be expressed as:
$$
D_{KL}(P \parallel Q) = H(P, Q) - H(P)
$$

Since \( H(P) \) is 0 for the true label, we get:
$$
D_{KL}(P \parallel Q) = H(P, Q)
$$

Thus, the cross-entropy loss \( H(P, Q) \) is equivalent to the KL divergence \( D_{KL}(P \parallel Q) \) in the context of binary classification with a Bernoulli distribution. This means minimizing the cross-entropy loss is equivalent to minimizing the KL divergence between the true distribution and the predicted distribution.

### Summary

In summary, the cross-entropy loss in binary classification can be derived from the Bernoulli distribution and is directly related to the KL divergence. Specifically, the cross-entropy loss is the same as the KL divergence between the true distribution of the labels and the predicted distribution. Therefore, minimizing the cross-entropy loss in a machine learning model corresponds to minimizing the divergence between the true and predicted distributions, ensuring that the model's predictions are as close as possible to the true labels.

---
slide 15 - 21


loss function

- minimise the sum of misclassified samples
- we dont have 0, 1 cateogry and -1, 1 because this allows multiplying with class labels, neg being misclassified 

- if the sign function was included then we would jsut count the misclassification, and gradient would vanish everywhere sol..?

- include hinge loss: sum of all the samples, the receive if the value is larger then 0
- the derivative is not computable due to kink of the funciton
- subgradient can be used however.
- subgrdient: we always have a lower bound if multiply the gradient direction with difference of Xo and X
- g is not required to be unique as multiple can exist. the set of all the is called subdifferential
---

SVM computes the optimalyy seperating hyperplane, some plane that seperates two classes with the idea that it produces the maximum margin

---
L1 and L2 losses are common loss functions used in machine learning and statistics, each with its unique characteristics and applications. Below is a detailed comparison:

### L1 Loss (Mean Absolute Error)

**Formula:**
\[ \text{L1 Loss} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i| \]

**Characteristics:**
- **Robustness to Outliers:** L1 loss is more robust to outliers than L2 loss. This is because the absolute value does not amplify the error as much as squaring does.
- **Sparsity:** L1 loss tends to produce sparse solutions. In the context of linear regression, using L1 regularization (Lasso) can lead to many coefficients being exactly zero, effectively performing feature selection.
- **Optimization:** L1 loss can be harder to optimize than L2 loss because it is not differentiable at zero. However, subgradient methods can be used for optimization.

**Use Cases:**
- Problems where robustness to outliers is important.
- Situations where feature selection or sparsity is desired.

### L2 Loss (Mean Squared Error)

**Formula:**
\[ \text{L2 Loss} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 \]

**Characteristics:**
- **Sensitivity to Outliers:** L2 loss is more sensitive to outliers because squaring the error term increases the penalty for large errors.
- **Smoothness:** L2 loss results in a smooth, differentiable function, which is easier to optimize using gradient-based methods.
- **Closed-form Solution:** In linear regression, L2 regularization (Ridge) has a closed-form solution, making it computationally efficient.

**Use Cases:**
- Problems where the presence of outliers is minimal or where their effect needs to be pronounced.
- Situations where smooth optimization is preferred, such as in neural networks and linear regression.

### Comparison

| Feature                    | L1 Loss (MAE)                           | L2 Loss (MSE)                           |
|----------------------------|-----------------------------------------|-----------------------------------------|
| Robustness to Outliers     | High                                    | Low                                     |
| Solution Sparsity          | Can produce sparse solutions            | Generally produces dense solutions      |
| Optimization Difficulty    | Harder to optimize due to non-differentiability at zero | Easier to optimize due to smoothness     |
| Use in Regularization      | Leads to Lasso (feature selection)      | Leads to Ridge (smooth penalty)         |
| Error Magnification        | Linear magnification of error           | Quadratic magnification of error        |

### Choosing Between L1 and L2

- **If your data has many outliers:** Use L1 loss.
- **If you need a smooth optimization process:** Use L2 loss.
- **If you want to perform feature selection:** Use L1 regularization.
- **If your model can handle outliers or you want to penalize larger errors more severely:** Use L2 loss.

In practice, the choice between L1 and L2 loss (or a combination, as in Elastic Net) depends on the specific characteristics and requirements of the problem at hand.