single nueron is computing inner procudcts of its weights

a fully connected layer is nothing but a matirx multiplicaction, more speciifcall of matrix W and y


fully conntectd layer

$$


\begin{bmatrix} \hat{y_1} \\ \hat{y_2} \end{bmatrix}\ = \begin{bmatrix} \hat{w_11} & \hat{w_12} & \hat{w_13} \\ \hat{w_21} & \hat{w_22} & \hat{w_23} \end{bmatrix}\ \begin{bmatrix} \hat{x_1} \\ \hat{x_2} \\ \hat{x_3} \end{bmatrix}\
$$


to get the gradisent we need to partial deriatives. gradient wrt of wegiths for gradient descent 
and gradient wrt inputs for backpropagation



--- 

## Forward Pass:

In the forward pass, the input data propagates through the neural network layers, ultimately producing an output. Each layer applies a series of transformations to the input data using its weights and activation functions. Mathematically, the forward pass can be represented as follows:

For a given layer $l$, the output $z^{[l]}$ is computed as:
$$
z^{[l]} = W^{[l]} \cdot a^{[l-1]} + b^{[l]}
$$
Where:
- $W^{[l]}$ is the weight matrix of layer $l$.
- $a^{[l-1]}$ is the activation output from the previous layer.
- $b^{[l]}$ is the bias vector of layer $l$.
- $z^{[l]}$ is the pre-activation output of layer $l$.

Then, the activation output $a^{[l]}$ of layer $l$ is obtained by applying the activation function $g^{[l]}$:
$$
a^{[l]} = g^{[l]}(z^{[l]})
$$

This process continues through each layer until the final output layer.

## [[4. Back Propagation]]:

In the backward pass, the gradients of the loss function with respect to the parameters of the network are computed. These gradients are  used to update the weights and biases of the network using an optimization algorithm like gradient descent. Backpropagation calculates the gradients using the chain rule of calculus, propagating the error backwards through the network.

Mathematically, the backward pass involves computing the gradients of the loss function with respect to the parameters of the network. For the weights of a given layer $l$, the gradient is computed as:

$$
 \frac{\partial \mathcal{L}}{\partial W^{[l]}} = \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot \frac{\partial z^{[l]}}{\partial W^{[l]}} 
$$
Similarly, for the biases:
$$
\frac{\partial \mathcal{L}}{\partial b^{[l]}} = \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot \frac{\partial z^{[l]}}{\partial b^{[l]}}
$$
These gradients are then used in the optimization step to update the weights and biases of the network.

Now, regarding your question about the gradient with respect to inputs in backpropagation: It's crucial because it allows us to understand how changes in the inputs affect the loss function. This gradient helps in scenarios like adversarial attacks or understanding the importance of different features in the input data.

Mathematically, the gradient of the loss function with respect to the input of a given layer $l$ can be computed as:

$$\frac{\partial \mathcal{L}}{\partial a^{[l-1]}} = \frac{\partial \mathcal{L}}{\partial z^{[l]}} \cdot \frac{\partial z^{[l]}}{\partial a^{[l-1]}} $$

This gradient allows us to understand how changes in the input affect the loss function and consequently how to adjust the input to minimize the loss.



___
