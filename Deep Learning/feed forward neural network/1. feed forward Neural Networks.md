NN are universal approximation functions
retrospection function that would map any high dimension input to an inner product of weight metric and the input. then sign distance is computed

we can not solve problems with single perceptions because non linearity is necessary e.g XOR

Multi layer perceptron enables non linear boundaries
input ,
hidden (hidden because they do not immediately observe the input, weights in the hidden layers are not directly observable  ),
output layers 

Universal Approximation Theorem
With a single hidden layer we can approximate any continous funtions
![[Screenshot from 2024-05-19 13-27-21.png]]
F(x) is an aproximation which is a iinear combination of non-linearities, that are computed from linear combonations
if we increase N the epsilon goes down, with N to infn to epsilon will approach zero


by building deeper networks we try to simply function and the power of the representatin function to get better processing towards he decision

with going deeper for layers, it helps us in decompostion in the problem and it is more effecient then using a very long single hidden layer - ineach step (layer) we simplify the function

[[2. activation functions]]

[[one hot encoding]]

[[3. loss functions]]

[[3. Optimising Problem]]

[[5. layer abstraction]]


