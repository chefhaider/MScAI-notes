  ## softmax function aka normalised exponential function

$$
\hat{y}_i = \frac{exp(x_k)}{\sum_{j=1}^k exp(x_j)}
$$

give ouput probs for each of the classes
use exponentioal func to map everything to positive space.
Kolmogorov's axioms properties:
- on summing up all elements it will equal to one
- have probablitic output towards 0 or 1.  


### Sign Acitvation function
not good to use in combination with gradient descent because its zero everywhere except at point zero, and there you have infinity as values
- The sign function, also known as the signum function, is a simple function that extracts the sign of a real number. It is defined as:
    


### Sigmoid Function
the derivative can be computed very easily.

problem as soon as we go away futher away towards the converging line the derivative of the funciton approaches zero or it "Saturates", and then couple of sigmoid finctions can lead to vanishin gradient problems 

sol?

### ReLU (Piecewise-linear activation function )
fixes the problem of sigmoid for vanishing gradient, ReLU does not saturate in the positive region.