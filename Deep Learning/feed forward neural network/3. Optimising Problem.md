taking minimum of the [[3. loss functions]] to optimise the weight

[[3. Gradient Descent]]

 ![[Screenshot from 2024-05-19 14-53-14.png]]
initialise W and iterate till convergance, special n, learning rate tells how long the steps individual errors are

will always find minima it maybe local however
one technique use random initialisation to find multiple local minima and choose the best.

[[4. Back Propagation]]
