calculating [[partial derivatives]]


- for every neuron we have input x1 and x2 and output y' 
- now the forward pass, evaluate the loss function 
- calculate derivative wrt y' that comes in the backwards pass
- for trainable weights we would also need to compute the derivatives in order to compute the parameter updates

*see slide 31 for back propagation example*


[[Stability problem]]

Positive Feedback or Negative Feedback  occuring due multiplying very high and low values in packpropagation.

![[Screenshot from 2024-05-19 15-30-32.png]]
concepts relates to the learning rate, how we set eta to high or too low


products of partial causes vanishing or explosing gradient
products of partial causes numerical errors to multiply

[[Finite Differences]]
h is a very small increment to the x

$$
f'(x) = \lim_{h \to 9} \frac{f(x+h) - f(x)}{h}
$$


and we also have a symmetric definition SEE.

easy to use computationally inefficient.

[[Analytic Gradient]]
![[Screenshot from 2024-05-19 15-10-39.png]]




back propogation is not a training algorithim it is just a way of computing gradient