

### Batch Gradient Descent
Batch gradient descent iterates over all samples and is preferred for convex optimization problems. - mmeory limitations

### Stochastic Gradient Descent (SGD)
For non-convex problems or memory-limited scenarios, Stochastic Gradient Descent is preferred. It uses only one sample per iteration and is highly parallelizable.

### Minibatch SGD
A compromise between batch and stochastic gradient descent, minibatch SGD randomly selects M samples. This method aids in regularization by preventing overfitting. While optimization problems may have exponential local minima and be non-convex, many local minima are close to the global minimum. The global minimum, based on the training set, may even be better than local minima as it provides a more generalized solution.

### Learning Rate Choice
Ideal learning rate strategies include:
- Starting with a learning rate, e.g., n=0.01, and dividing by 10 every x epochs.
- Line search: finding the optimal eta (n) at each epoch, although this method is noisy.
- Second-order methods like the Hessian method, which are computationally expensive.
- Accelerating in the direction of persistent gradients by keeping track of the weighted sum over the last few gradient directions. This method reduces oscillations and accelerates the optimization procedure.
- **Nesterov** Accelerated Gradient/Momentum, which performs a look-ahead by adding momentum before computing the gradient, directly moving towards the desired minimum without alternating, and therefore accelerates convergence.

### Individual Parameter Adjustments
If some features are activated often while others are not, individual parameters in the network are needed. Large parameters for infrequent features and small ones for frequent ones.
- Fix: **AdaGrad**, which allows individual learning rates for all weights.
  - Compute the gradient g(k), then take the product (⊙) with itself to estimate the element-wise variance in a variable r, representing its standard deviation.
- Problem: Weight decay may occur too early.
- Fix: Introduce a constant p to consider when evaluating r. This is **RMSProp**.
- **Adadelta** further improves on this by eliminating the need to set a learning rate.
- **Adam** Also uses the gradient direction method, in addition to momentum term we have r term which tries to steer the learning individually for each dimension. and additionally we have bias correction where v is scaled by 1-mew (i.e u). and r is scaled b 1-row(i.e p)
- Adam fails to converge to an optimal solution, Adam and similar problems do not guarantee for convex problems.
- fix: **AMSGrad** by adding a mximum over the moemntum term max(v_hat^(k-1),v^k)



----

To understand the intuition behind Equation 7 of the Adam optimizer, where the parameters are updated, it's helpful to break down what each component represents and why they are combined in the way they are.
### Intuition Behind Adam Update Step

1. **First Moment Estimate (\(\hat{m}_t\)):**

   The first moment estimate \(\hat{m}_t\) is essentially the moving average of the gradients. This estimate helps to smooth out the noise in the gradients, which can come from the stochastic nature of the optimization process (i.e., using mini-batches).

   $$
   \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
   $$

   This bias correction ensures that in the initial steps (when \(t\) is small), the estimate does not start off too small because the moving average \(m_t\) would initially have lower magnitude due to zero initialization.

2. **Second Moment Estimate (\(\hat{v}_t\)):**

   The second moment estimate \(\hat{v}_t\) is the moving average of the squared gradients. This estimate captures the variance of the gradients, indicating how much the gradient is changing.

   $$
   \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
   $$

   Similar to the first moment, the bias correction ensures that the estimate starts off with a proper scale, preventing it from being too small in the early iterations.

3. **Parameter Update Equation:**

   $$
   \theta_t = \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
   $$

### Intuition Behind Combining \(\hat{m}_t\) and \(\sqrt{\hat{v}_t}\):

- **\(\hat{m}_t\) (First Moment / Mean):** This term represents the direction and magnitude of the gradient. If \(\hat{m}_t\) is large, it means the gradient has consistently been pointing in a particular direction, suggesting a strong trend that the optimizer should follow.

- **\(\sqrt{\hat{v}_t}\) (Second Moment / Variance):** This term represents the variability of the gradients. Taking the square root transforms the squared gradients back into the same scale as the original gradients. High variance (large \(\sqrt{\hat{v}_t}\)) indicates that the gradient is fluctuating a lot, and the optimizer should be cautious, taking smaller steps to prevent overshooting.

- **Dividing by \(\sqrt{\hat{v}_t}\):** By dividing the first moment by the second moment, Adam normalizes the gradient. This normalization helps to scale the learning rate for each parameter individually, making the learning rate smaller for parameters with high variance and larger for parameters with low variance. This adaptive scaling prevents the optimizer from taking too large steps in directions with high uncertainty (high gradient variance) and allows it to take larger steps in directions with low uncertainty.

- **Adding \(\epsilon\):** The term \(\epsilon\) is added to the denominator to ensure numerical stability, preventing division by zero or extremely small numbers which could result in excessively large updates.



---

Choosing between Adam and Adadelta depends on the specific characteristics of your optimization problem and your computational resources. Here are the pros and cons of each:

### Adam

#### Pros:

1. **Adaptive Learning Rates**: Adam dynamically adjusts the learning rates for each parameter based on estimates of the first and second moments of the gradients.
2. **Bias Correction**: Adam incorporates bias correction, which helps in stabilizing the optimization process, especially in the initial training phases.
3. **Efficient**: Adam is computationally efficient and typically converges faster than traditional stochastic gradient descent methods.

#### Cons:

1. **Sensitive to Hyperparameters**: Adam has several hyperparameters (e.g., β1, β2, ε), and the performance can be sensitive to their values. Tuning these hyperparameters might be necessary for optimal performance.
2. **Memory Intensive**: Adam maintains momentum and the moving average of squared gradients for each parameter, which can be memory-intensive, especially for large models with many parameters.

### Adadelta

#### Pros:

1. **No Learning Rate Hyperparameter**: Adadelta eliminates the need to manually set a learning rate, making it less sensitive to hyperparameters.
2. **Memory Efficient**: Adadelta requires less memory compared to Adam because it doesn't store the exponentially decaying average of past squared gradients.
3. **Convergence**: Adadelta is known for its fast convergence, especially in scenarios where the learning rates need to be adjusted dynamically.

#### Cons:

1. **Lack of Momentum**: Adadelta does not include momentum, which might slow down convergence in some cases, especially when the gradients are sparse.
2. **Slower Convergence in Certain Cases**: While Adadelta converges quickly in many cases, it might converge slower compared to Adam in some scenarios, especially when the learning rates need to adapt rapidly.

### When to Choose Adam over Adadelta:

- **Large Datasets**: Adam might be more suitable for large datasets where efficient convergence is crucial.
- **More Control Over Hyperparameters**: If you have the resources to tune hyperparameters effectively, Adam could be preferred due to its potentially faster convergence rate.
- **Stabilizing Training**: Adam's bias correction mechanism can help stabilize training, especially in the early stages.

### When to Choose Adadelta over Adam:

- **Limited Computational Resources**: Adadelta might be preferred when memory and computational resources are limited, as it is more memory-efficient.
- **Less Sensitivity to Hyperparameters**: If you prefer a method with fewer hyperparameters to tune and less sensitivity to their values, Adadelta might be a better choice.
- **Sparse Data**: Adadelta might perform better with sparse data or in scenarios where the gradients are noisy or irregular.
---

### Distributed gradient Descend